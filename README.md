Augury Project â€” Quick-start Guide

This README walks through cloning the repo, installing dependencies, and running the augury pipeline locally on an Apple-silicon MacBook Pro.

â¸»

1 Â· Clone the repo

git clone <your-github-url> augury
cd augury

2 Â· Python & virtual-env
	â€¢	macOS ships with Python 3.11+, but keep it isolated:

python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip

3 Â· Install packages

pip install "llama-cpp-python~=0.2.65"  # Metal wheel for Apple Silicon
pip install pyyaml fastapi uvicorn pytest rich  # rule engine + API + tests

(If an Intel/Linux box with CUDA is used, install llama-cpp-python[cu121] instead.)

Tip: create requirements.txt later so this becomes pip install -r requirements.txt.

4 Â· Download the quantised model

Create a local models/ folder and drop the GGUF file in it.

mkdir -p models
curl -L -o models/llama3-8B-q4_k_m.gguf \
  https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf

Size â‰ˆ 5 GB. AirDrop or external SSD if download bandwidth is painful.

5 Â· Run unit tests (sanity check)

PYTHONPATH=. pytest tests/

All three tests in tests/test_rules.py should pass.

6 Â· Generate an omen from CLI

echo '{"species":"corvus corax","side":"dexter"}' | AUGUR_SILENT=1 python augur.py

	â€¢	AUGUR_SILENT=1 suppresses low-level Metal logs.
	â€¢	Output should be a theatrical proclamation.

7 Â· Start the FastAPI service (optional)

uvicorn api:app --reload --port 8000

Visit http://localhost:8000/docs for an interactive Swagger UI. Example curl:

curl -X POST http://localhost:8000/proclaim \
     -H "Content-Type: application/json" \
     -d '{"species":"corvus corax","side":"dexter"}'

8 Â· Troubleshooting

Symptom	Fix
Metal kernel spam	Ensure AUGUR_SILENT=1 or keep default augur.py which hard-mutes logs.
Out-of-memory on 8 GB MacBook Air	Swap to smaller Q3_K_S model (â‰ˆ 3 GB).
CUDA box returns loadLibrary failed	Re-install llama-cpp-python[cu121] inside an environment with CUDA 12.1 libs.

9 Â· GPU tower usage  (RTX 3090 / dual 4090)

# inside venv on the tower
pip install "llama-cpp-python[cu121]~=0.2.65"
AUGUR_SILENT=1 python augur.py < input.json

Tune n_gpu_layers in augur.py to ~60 for 3090, ~70-72 for dual 4090.

10 Â· Next milestones
	1.	Containerise (Dockerfile, multi-arch buildx).
	2.	Vision & audio nodes â†’ Redis Streams.
	3.	Streamlit dashboard for live performance mode.

â¸»

ðŸ—’ Note to both developers: keep rule additions in augury_rules.yaml and write a new pytest whenever you encode a fresh auspice rule. 100 % test pass rate keeps the oracle honest.
